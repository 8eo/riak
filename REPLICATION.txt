Riak Replication
================

Riak replication copies all data from a primary cluster to a secondary. Currently it
is unidirectional, although you can configure a pair of connection to sync
bidirectionally between two clusters.

IMPORTANT: If nodes have been joined to the cluster since it was started, the cluster
must be restarted before replication can be set up.  A rolling restart will work fine.
This is a known issue and will be fixed in a future release.

On the primary cluster, you need to set up "listeners" on an IP address/port accessible
from the secondary clusters:

    % cd $RIAK; bin/riak-repl add-listener dev1@127.0.0.1 192.168.1.1 9010

This makes the node dev1@127.0.0.1 listen on 192.168.1.1:9010 for connections from
secondary clusters.

On any node in the secondary cluster, tell it where to find the primary cluster (called
site1).
  
     % cd $RIAK; bin/riak-repl add-site 192.168.1.1 9010 site1

The secondary cluster will connect to the primary which initiates a full-sync.  Meanwhile
any writes on the primary will also be streamed to the secondary cluster.  The clusters
periodically do a full sync.  The interval is configured in the riak_repl section of the
app.config on the primary cluster - {fullsync_interval, Minutes} sets how long to wait 
after a full-sync completes before another is started.  The default is set to 6 hours
(360 minutes).

To remove listeners from a node in the primary cluster

    % cd $RIAK; bin/riak-repl del-listener dev1@127.0.0.1 192.168.1.1 9010

And to stop replication on the secondary cluster

     % cd $RIAK; bin/riak-repl del-site site1

As well as configuring replication, the 'bin/riak-repl status' provides counts on
how much data was transmitted.
   
     % cd $RIAK; bin/riak-repl status
 

Known Issues

 * If the node elected as replication leader in the secondary goes down, a new node is elected
   but it does not successfully reconnect to the primary cluster.  The workaround is to restart
   all nodes in the secondary cluster.

 * As described above, the list of candidates for replication leaders is not updated when
   nodes are added/removed from the ring.  The workaround is to restart all nodes in the cluster
   (a rolling restart works fine).

 * Adding/dropping listeners and sites quickly crashes riak_repl_controller. It recovers
   automatically.

